{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from parse_annotation import parse_annotation\n",
    "from somajo import SoMaJo as smj\n",
    "import jellyfish\n",
    "import math\n",
    "import trankit\n",
    "\n",
    "tokenizer = smj(language=\"en_PTB\")\n",
    "BASE_DATA_PATH = \"/home/burak/Desktop/thesis/code/IM/twitter_data\"\n",
    "THREADS_PATH = BASE_DATA_PATH + \"/raw\"\n",
    "TOKENIZED_THREADS_PATH = BASE_DATA_PATH + \"/raw/tokenized\"\n",
    "ANN_PATH = BASE_DATA_PATH + \"/ann/raw\"\n",
    "FIXED_ANN_PATH = BASE_DATA_PATH + \"/ann/fixed_raw\"\n",
    "file_name = \"514_947136019320651777.branch1.txt.username_text_tabseparated\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(THREADS_PATH)\n",
    "\n",
    "readable_annotations = dict()\n",
    "\n",
    "for file in files:\n",
    "    try:\n",
    "        if file[-12:] == 'tabseparated':\n",
    "            readable_annotations[file] = list()\n",
    "            for annotation in parse_annotation(\n",
    "                THREADS_PATH + '/' + file,\n",
    "                FIXED_ANN_PATH + '/' + file,\n",
    "                extract_16=False\n",
    "            ):\n",
    "                readable_annotations[file].append(annotation)\n",
    "    except Exception as inst:\n",
    "        print(file, type(inst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conll_format_dict = dict()\n",
    "for file_name in readable_annotations.keys():\n",
    "    file_annotations = readable_annotations[file_name]\n",
    "    conll_format_dict[file_name] = []\n",
    "    \n",
    "    token_index = 0\n",
    "    tokens = []\n",
    "    with open(TOKENIZED_THREADS_PATH + '/' + file_name) as f:\n",
    "        text = f.read()\n",
    "        tokenized_thread = json.loads(text)\n",
    "        for sentence_index, sentence in enumerate(tokenized_thread['sentences']):\n",
    "            for token_sentence_index, token in enumerate(sentence['tokens']):\n",
    "                tokens.append([\n",
    "                    token['characterOffsetBegin'],\n",
    "                    token['characterOffsetEnd'],\n",
    "                    len(tokens),\n",
    "                    sentence_index,\n",
    "                    token_sentence_index,\n",
    "                    token['surface']\n",
    "                ])\n",
    "    \n",
    "    for annotation_index, annotation in enumerate(file_annotations):\n",
    "        conll_annotation = dict()\n",
    "        \n",
    "        index_cursor = 0\n",
    "        \n",
    "        # arg1\n",
    "        arg1 = dict()\n",
    "        arg1['TokenList'] = []\n",
    "        arg1['RawText'] = []\n",
    "        arg1['CharacterSpanList'] = []\n",
    "        \n",
    "        if annotation['arg1_range'] != '':\n",
    "            for i, arg1_range_part in enumerate(annotation['arg1_range'].split(';')):\n",
    "                arg1_range_part_start = int(arg1_range_part.split('..')[0])\n",
    "                arg1_range_part_end = int(arg1_range_part.split('..')[1])\n",
    "                arg1['CharacterSpanList'].append([int(range_offset) for range_offset in arg1_range_part.split('..')])\n",
    "                arg1_range_text = annotation['arg1'][i]\n",
    "                arg1['RawText'].append(arg1_range_text)\n",
    "                \n",
    "                # find token indexes for arg1\n",
    "                \n",
    "                start_index = None\n",
    "                end_index = None\n",
    "                \n",
    "                #find start\n",
    "                for i in range(len(tokens)):\n",
    "                    if arg1_range_part_start == tokens[i][0]:\n",
    "                        start_index = i \n",
    "                        break\n",
    "                    if arg1_range_part_start > tokens[i][0] and arg1_range_part_start < tokens[i][1]:\n",
    "                        start_index = i\n",
    "                        print(tokens[i])\n",
    "                        print(arg1_range_part_start)\n",
    "                        print('arg1 start not equal')\n",
    "                        break\n",
    "                    if arg1_range_part_start == tokens[i][1]:\n",
    "                        start_index = i + 1\n",
    "                        print(tokens[i + 1])\n",
    "                        print(tokens[i])\n",
    "                        print(arg1_range_part_start)\n",
    "                        print('arg1 start exceeded')\n",
    "                        print(file_name)\n",
    "                        break\n",
    "                        \n",
    "                if start_index == None:\n",
    "                    print(arg1_range_part)\n",
    "                    print(tokens)\n",
    "                    print('arg1 start in', file_name)\n",
    "                    continue\n",
    "                    \n",
    "                \n",
    "                #find end\n",
    "                for i in range(len(tokens)):\n",
    "                    if arg1_range_part_end == tokens[i][1]:\n",
    "                        end_index = i \n",
    "                        break\n",
    "                    if arg1_range_part_end > tokens[i][0] and arg1_range_part_end < tokens[i][1]:\n",
    "                        end_index = i\n",
    "                        print(tokens[i])\n",
    "                        print(arg1_range_part_end)\n",
    "                        print('arg1 end not equal')\n",
    "                        break\n",
    "                    if arg1_range_part_end == tokens[i][0]:\n",
    "                        end_index = i - 1\n",
    "                        print(tokens[i - 1])\n",
    "                        print(tokens[i])\n",
    "                        print(arg1_range_part_end)\n",
    "                        print('arg1 end exceeded 1')\n",
    "                        break\n",
    "                    if arg1_range_part_end < tokens[i][0] and arg1_range_part_end < tokens[i][1]:\n",
    "                        end_index = i - 1\n",
    "                        print(tokens[i - 1])\n",
    "                        print(tokens[i])\n",
    "                        print(arg1_range_part_end)\n",
    "                        print('arg1 end exceeded 2')\n",
    "                        print(file_name)\n",
    "                        break\n",
    "\n",
    "                if end_index == None:\n",
    "                    print(arg1_range_part)\n",
    "                    print(tokens)\n",
    "                    print('arg1 end error in', file_name)\n",
    "                    continue\n",
    "                \n",
    "                for t in tokens[start_index : end_index + 1]:\n",
    "                    arg1['TokenList'].append(t[0:5])\n",
    "            \n",
    "        \n",
    "        \n",
    "        # arg2\n",
    "        arg2 = dict()\n",
    "        arg2['TokenList'] = []\n",
    "        arg2['RawText'] = []\n",
    "        arg2['CharacterSpanList'] = []\n",
    "        \n",
    "        if annotation['arg2_range'] != '':\n",
    "            for i, arg2_range_part in enumerate(annotation['arg2_range'].split(';')):\n",
    "                arg2_range_part_start = int(arg2_range_part.split('..')[0])\n",
    "                arg2_range_part_end = int(arg2_range_part.split('..')[1])\n",
    "                arg2['CharacterSpanList'].append([int(range_offset) for range_offset in arg2_range_part.split('..')])\n",
    "                arg2_range_text = annotation['arg2'][i]\n",
    "                arg2['RawText'].append(arg2_range_text)\n",
    "                \n",
    "                # find token indexes for arg1\n",
    "                \n",
    "                start_index = None\n",
    "                end_index = None\n",
    "                \n",
    "                #find start\n",
    "                for i in range(len(tokens)):\n",
    "                    if arg2_range_part_start == tokens[i][0]:\n",
    "                        start_index = i\n",
    "                        break\n",
    "                    if arg2_range_part_start > tokens[i][0] and arg2_range_part_start < tokens[i][1]:\n",
    "                        start_index = i\n",
    "                        print(tokens[i])\n",
    "                        print(arg2_range_part_start)\n",
    "                        print('arg2 start not equal')\n",
    "                        break\n",
    "                    if arg2_range_part_start == tokens[i][1]:\n",
    "                        start_index = i + 1\n",
    "                        print(tokens[i + 1])\n",
    "                        print(tokens[i])\n",
    "                        print(arg2_range_part_start)\n",
    "                        print('arg2 start exceeded')\n",
    "                        print(file_name)\n",
    "                        break\n",
    "                \n",
    "                if start_index == None:\n",
    "                    print(arg2_range_part)\n",
    "                    print(tokens)\n",
    "                    print('arg2 start error in', file_name)\n",
    "                    continue\n",
    "                \n",
    "                #find end\n",
    "                for i in range(len(tokens)):\n",
    "                    if arg2_range_part_end == tokens[i][1]:\n",
    "                        end_index = i \n",
    "                        break\n",
    "                    if arg2_range_part_end > tokens[i][0] and arg2_range_part_end < tokens[i][1]:\n",
    "                        end_index = i\n",
    "                        print(tokens[i])\n",
    "                        print(arg2_range_part_end)\n",
    "                        print('arg2 end not equal')\n",
    "                        break\n",
    "                    if arg2_range_part_end == tokens[i][0]:\n",
    "                        end_index = i - 1\n",
    "                        print(tokens[i - 1])\n",
    "                        print(tokens[i])\n",
    "                        print(arg2_range_part_end)\n",
    "                        print('arg2 end exceeded 1')\n",
    "                        break\n",
    "                    if arg2_range_part_end < tokens[i][0] and arg2_range_part_end < tokens[i][1]:\n",
    "                        end_index = i - 1\n",
    "                        print(tokens[i - 1])\n",
    "                        print(tokens[i])\n",
    "                        print(arg2_range_part_end)\n",
    "                        print('arg2 end exceeded 2')\n",
    "                        print(file_name)\n",
    "                        break\n",
    "\n",
    "                if end_index == None:\n",
    "                    print(arg2_range_part)\n",
    "                    print(tokens)\n",
    "                    print('arg2 end error in', file_name)\n",
    "                    continue\n",
    "                \n",
    "                for t in tokens[start_index : end_index + 1]:\n",
    "                    arg2['TokenList'].append(t[0:5])\n",
    "        \n",
    "        # connective\n",
    "        connective = dict()\n",
    "        connective['TokenList'] = []\n",
    "        connective['RawText'] = []\n",
    "        connective['CharacterSpanList'] = []\n",
    "        \n",
    "        if '..' in annotation['connective_range']:\n",
    "            for i, connective_range_part in enumerate(annotation['connective_range'].split(';')):\n",
    "                connective_range_part_start = int(connective_range_part.split('..')[0])\n",
    "                connective_range_part_end = int(connective_range_part.split('..')[1])\n",
    "                connective['CharacterSpanList'].append([int(range_offset) for range_offset in connective_range_part.split('..')])\n",
    "                connective_range_text = annotation['connective'][i]\n",
    "                \n",
    "\n",
    "                if connective_range_text == '&amp':\n",
    "                    connective_range_text = '&amp;'\n",
    "                \n",
    "                connective['RawText'].append(connective_range_text)\n",
    "                \n",
    "                # find token indexes for connective\n",
    "                \n",
    "                start_index = None\n",
    "                end_index = None\n",
    "                \n",
    "                #find start\n",
    "                for i in range(len(tokens)):\n",
    "                    if connective_range_part_start == tokens[i][0]:\n",
    "                        start_index = i \n",
    "                        break\n",
    "                    if connective_range_part_start > tokens[i][0] and connective_range_part_start < tokens[i][1]:\n",
    "                        start_index = i\n",
    "                        print(tokens[i])\n",
    "                        print(connective_range_part_start)\n",
    "                        print('connective start not equal')\n",
    "                        break\n",
    "                    if connective_range_part_start == tokens[i][1]:\n",
    "                        start_index = i + 1\n",
    "                        print(tokens[i + 1])\n",
    "                        print(tokens[i])\n",
    "                        print(connective_range_part_start)\n",
    "                        print('connective start exceeded')\n",
    "                        print(file_name)\n",
    "                        break\n",
    "                        \n",
    "                if start_index == None:\n",
    "                    print(connective_range_part)\n",
    "                    print('connective start error in', file_name)\n",
    "                    continue\n",
    "                \n",
    "                #find end\n",
    "                for i in range(len(tokens)):\n",
    "                    if connective_range_part_end == tokens[i][1]:\n",
    "                        end_index = i \n",
    "                        break\n",
    "                    if connective_range_part_end > tokens[i][0] and connective_range_part_end < tokens[i][1]:\n",
    "                        end_index = i\n",
    "                        print(tokens[i])\n",
    "                        print(connective_range_part_end)\n",
    "                        print('connective end not equal')\n",
    "                        break\n",
    "                    if connective_range_part_end == tokens[i][0]:\n",
    "                        end_index = i - 1\n",
    "                        print(tokens[i - 1])\n",
    "                        print(tokens[i])\n",
    "                        print(connective_range_part_end)\n",
    "                        print('connective end exceeded 1')\n",
    "                        break\n",
    "                    if connective_range_part_end < tokens[i][0] and connective_range_part_end < tokens[i][1]:\n",
    "                        end_index = i - 1\n",
    "                        print(tokens[i - 1])\n",
    "                        print(tokens[i])\n",
    "                        print(tokens)\n",
    "                        print(connective_range_part_end)\n",
    "                        print('connective end exceeded 2')\n",
    "                        print(file_name)\n",
    "                        break\n",
    "\n",
    "                if end_index == None:\n",
    "                    print('connective error in', file_name)\n",
    "                    continue\n",
    "                \n",
    "                for t in tokens[start_index : end_index + 1]:\n",
    "                    connective['TokenList'].append(t[0:5])\n",
    "        \n",
    "        relation = {\n",
    "            'Arg1': arg1,\n",
    "            'Arg2': arg2,\n",
    "            'Connective': connective,\n",
    "            'Sense': [annotation['sense']],\n",
    "            'Type': annotation['type'],\n",
    "            'DocID': file_name,\n",
    "            'ID': file_name[0:3] + str(annotation_index)\n",
    "        }\n",
    "        conll_format_dict[file_name].append(relation)\n",
    "\n",
    "json_string = json.dumps(conll_format_dict)\n",
    "with open('./conll_relations_raw_fixed_spans.json', 'w') as outfile:\n",
    "    outfile.write(json_string)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# find token indexes for arg2\n",
    "for index_window in range(len(tokens) - len(arg1_tokens) + 1):\n",
    "    distance = jellyfish.levenshtein_distance(\n",
    "        ' '.join(arg1_tokens),\n",
    "        ' '.join(tokens_text[index_window : index_window + len(arg1_tokens)])\n",
    "    )\n",
    "    if distance <= 2:\n",
    "        arg1_token_index = index_window\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_distance = math.inf\n",
    "best_option = None\n",
    "for index_window in range(len(tokens) - len(arg1_tokens) + 1):\n",
    "    distance = jellyfish.levenshtein_distance(\n",
    "        ' '.join(arg1_tokens),\n",
    "        ' '.join(tokens_text[index_window : index_window + len(arg1_tokens)])\n",
    "    )\n",
    "    if (distance < best_distance):\n",
    "        best_distance = distance\n",
    "        arg1_token_index = index_window\n",
    "\n",
    "    if distance <= 2:\n",
    "        arg1_token_index = index_window\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
