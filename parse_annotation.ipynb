{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d7dbe9d",
   "metadata": {
    "code_folding": [
     38,
     49
    ]
   },
   "outputs": [],
   "source": [
    "### works for twitter data\n",
    "def parse_annotation(input_file_path, annotated_file_path, extract_16=True, check_intertweet=False):\n",
    "    extract = None\n",
    "    if extract_16:\n",
    "        extract = extract_from_ranges_16\n",
    "    else:\n",
    "        extract = extract_from_ranges\n",
    "    annotations = []\n",
    "    with open(input_file_path, encoding='utf8') as input_file:\n",
    "        with open(annotated_file_path) as ann_file:\n",
    "            try:\n",
    "                file_content = input_file.read()#decode('ISO-8859-1')\n",
    "            except Exception as inst:\n",
    "                print(type(inst))\n",
    "            for line in ann_file:\n",
    "                properties = line.split('|')\n",
    "                typ = properties[0]\n",
    "                connective_ranges = None\n",
    "                connective = None\n",
    "                if typ == 'Explicit':\n",
    "                    connective_ranges = properties[1]\n",
    "                    try:\n",
    "                        connective = extract(file_content, connective_ranges)\n",
    "                    except Exception as inst:\n",
    "                        connective = '--ERROR--'\n",
    "                if typ == 'Implicit' or typ == 'Hypophora':\n",
    "                    connective_ranges = properties[31]\n",
    "                sense = properties[8]\n",
    "                arg1_range = properties[14]\n",
    "                arg2_range = properties[20]\n",
    "                \n",
    "                intertweet = None\n",
    "                if check_intertweet:\n",
    "                    try:\n",
    "                        intertweet = is_intertweet([arg1_range, arg2_range, connective_ranges], file_content)\n",
    "                    except Exception as inst:\n",
    "                        interweet = '--ERROR--'\n",
    "                try:\n",
    "                    arg1 = extract(file_content, arg1_range)\n",
    "                except Exception as inst:\n",
    "                    arg1 = '--ERROR--'\n",
    "                try:\n",
    "                    arg2 = extract(file_content, arg2_range)\n",
    "                except Exception as inst:\n",
    "                    arg2 = '--ERROR--'\n",
    "                result = {\n",
    "                    'file_id': input_file_path,\n",
    "                    'arg1_range': arg1_range,\n",
    "                    'arg2_range': arg2_range,\n",
    "                    'arg1': arg1,\n",
    "                    'arg2': arg2,\n",
    "                    'connective_range': connective_ranges,\n",
    "                    'connective': connective,\n",
    "                    'sense': sense,\n",
    "                    'type': typ,\n",
    "                    'intertweet': intertweet\n",
    "                }\n",
    "                if check_intertweet:\n",
    "                    result['intertweet'] = intertweet\n",
    "                annotations.append(result)\n",
    "    return annotations\n",
    "\n",
    "def parse_annotation_pure(annotated_file_path):\n",
    "    annotations = []\n",
    "    with open(annotated_file_path) as ann_file:\n",
    "        for line in ann_file:\n",
    "            properties = line.split('|')\n",
    "            typ = properties[0]\n",
    "            connective_ranges = None\n",
    "            if typ == 'Explicit':\n",
    "                connective_ranges = properties[1]\n",
    "            if typ == 'Implicit' or typ == 'Hypophora':\n",
    "                connective_ranges = properties[31]\n",
    "            sense = properties[8]\n",
    "            arg1_range = properties[14]\n",
    "            arg2_range = properties[20]\n",
    "\n",
    "            result = {\n",
    "                'file_id': annotated_file_path,\n",
    "                'arg1_range': arg1_range,\n",
    "                'arg2_range': arg2_range,\n",
    "                'connective_range': connective_ranges,\n",
    "                'sense': sense,\n",
    "                'type': typ\n",
    "            }\n",
    "            annotations.append(result)\n",
    "    return annotations\n",
    "\n",
    "    \n",
    "\n",
    "def parse_tweets(file_id, annotations, raw_file_folder, input_file_folder):\n",
    "    tweets = []\n",
    "    with open(raw_file_folder + '/' + file_id, 'rb') as raw_file:\n",
    "        with open(input_file_folder + '/' + file_id, encoding='utf8') as input_file:\n",
    "            raw_file_text = raw_file.read()\n",
    "            input_file_text = input_file.read()\n",
    "            raw_utf8 = raw_file_text.decode('utf-8')\n",
    "            raw_iso = input_file_text\n",
    "            whole_text = raw_iso.encode('utf-16le')\n",
    "            raw_file_lines = raw_utf8.split('\\n')\n",
    "            tweet_texts = []\n",
    "            for raw_file_line in raw_file_lines[:-1]:\n",
    "                tokens = raw_file_line.split('\\t')[1].split()\n",
    "                tokens_to_be_removed = []\n",
    "                for token in tokens:\n",
    "                    if token[0] == '@':\n",
    "                        tokens_to_be_removed.append(token)\n",
    "                    else:\n",
    "                        break\n",
    "                for token in tokens_to_be_removed:\n",
    "                    tokens.remove(token)\n",
    "                tweet_texts.append(' '.join(tokens))\n",
    "            # :-1 because raw files has extra one empty line\n",
    "            for line_number, line in enumerate(raw_file_lines[:-1]):\n",
    "                poster = line.split('\\t')[0]\n",
    "                tweet_text = tweet_texts[line_number]\n",
    "                tweets.append({\n",
    "                    'poster': poster,\n",
    "                    'tweet_text': tweet_text,\n",
    "                    'arg1': set(),\n",
    "                    'arg2': set(),\n",
    "                    'connective': set(),\n",
    "                })\n",
    "\n",
    "            for annotation_index, annotation in enumerate(annotations):\n",
    "                # here, the annotations are going to be identified with their indexes in the annotation file\n",
    "                # arg1\n",
    "                for i, arg1_range_part in enumerate(annotation['arg1_range'].split(';')):\n",
    "                    if len(arg1_range_part) > 0:\n",
    "                        arg1_range_part_start = int(arg1_range_part.split('..')[0])\n",
    "                        arg1_range_part_end = int(arg1_range_part.split('..')[1])\n",
    "                        start_line = whole_text[:arg1_range_part_start * 2].decode('utf-16le').count('\\n')\n",
    "                        end_line = whole_text[:arg1_range_part_end * 2].decode('utf-16le').count('\\n')\n",
    "                        tweets[start_line]['arg1'].add(annotation_index)\n",
    "                        tweets[end_line]['arg1'].add(annotation_index)\n",
    "\n",
    "                # arg2\n",
    "                for i, arg2_range_part in enumerate(annotation['arg2_range'].split(';')):\n",
    "                    if len(arg2_range_part) > 0:\n",
    "                        arg2_range_part_start = int(arg2_range_part.split('..')[0])\n",
    "                        arg2_range_part_end = int(arg2_range_part.split('..')[1])\n",
    "                        start_line = whole_text[:arg2_range_part_start * 2].decode('utf-16le').count('\\n')\n",
    "                        end_line = whole_text[:arg2_range_part_end * 2].decode('utf-16le').count('\\n')\n",
    "                        tweets[start_line]['arg2'].add(annotation_index)\n",
    "                        tweets[end_line]['arg2'].add(annotation_index)\n",
    "\n",
    "                # connective\n",
    "                if (annotation['type'] == 'Explicit'):\n",
    "                    for i, connective_range_part in enumerate(annotation['connective_range'].split(';')):\n",
    "                        connective_range_part_start = int(connective_range_part.split('..')[0])\n",
    "                        start_line = whole_text[:connective_range_part_start * 2].decode('utf-16le').count('\\n')\n",
    "                        tweets[start_line]['connective'].add(annotation_index)\n",
    "\n",
    "                        connective_range_part_end = int(connective_range_part.split('..')[1])\n",
    "                        end_line = whole_text[:connective_range_part_end * 2].decode('utf-16le').count('\\n')\n",
    "                        tweets[end_line]['connective'].add(annotation_index)\n",
    "    return tweets\n",
    "    \n",
    "def extract_from_ranges_16(file_content, rangs):\n",
    "    file_content = file_content.encode('utf-16le')\n",
    "    text = []\n",
    "    for rang in rangs.split(';'):\n",
    "        if len(rang) < 2:\n",
    "            return ''\n",
    "        splitted = rang.split('..')\n",
    "        part_text = file_content[int(splitted[0]) * 2: int(splitted[1]) * 2].decode('utf-16le')\n",
    "        text.append(part_text)\n",
    "        \n",
    "    return text\n",
    "\n",
    "def extract_from_ranges(file_content, rangs):\n",
    "    text = []\n",
    "    for rang in rangs.split(';'):\n",
    "        if len(rang) < 2:\n",
    "            return ''\n",
    "        splitted = rang.split('..')\n",
    "        part_text = file_content[int(splitted[0]): int(splitted[1])]\n",
    "        text.append(part_text)\n",
    "        \n",
    "    return text\n",
    "\n",
    "def is_intertweet(ranges, file_content):\n",
    "    offsets = []\n",
    "    for _range in ranges:\n",
    "        if _range == None:\n",
    "            continue\n",
    "        for cur_offsets in _range.split(';'):\n",
    "            if len(cur_offsets) < 2:\n",
    "                return ''\n",
    "            splitted = cur_offsets.split('..')\n",
    "            for offset in splitted:\n",
    "                offsets.append(offset)\n",
    "    min_offset = min(offsets)\n",
    "    max_offset = max(offsets)\n",
    "    \n",
    "    file_content = file_content.encode('utf-16le')\n",
    "    substr = file_content[int(min_offset) * 2: int(max_offset) * 2].decode('utf-16le')\n",
    "    i = substr.find('\\n')\n",
    "    return i >= 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd7a2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "### works for PDTB\n",
    "def parse_relations(relations_file_path):\n",
    "    relations = []\n",
    "    with open(relations_file_path, encoding=\"utf-8\") as relations_file:\n",
    "        for relation_line in relations_file.readlines():\n",
    "            relation_json = json.loads(relation_line)\n",
    "            relation = {\n",
    "                'arg1': relation_json['Arg1']['RawText'],\n",
    "                'arg2': relation_json['Arg2']['RawText'],\n",
    "                'connective': relation_json['Connective']['RawText'],\n",
    "                'sense': relation_json['Sense'][0],\n",
    "                'type': relation_json['Type']\n",
    "            }\n",
    "            relations.append(relation)\n",
    "    return relations"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
